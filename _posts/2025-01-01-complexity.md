---
layout: post
title: "ComplexitÃ©"
subtitle: "introduction Ã  la notion de complexitÃ© d'algorithme."
date: 2020-01-26 23:45:13 -0400
background: '/img/posts/complexity/complexity.png'
---

# Introduction sur complexitÃ© algorithmique

### Quâ€™est-ce quâ€™une complexitÃ© algorithmique ?

Câ€™est un outil ou modÃ¨le mathÃ©matique important en science informatique qui permet dâ€™estimer les performances asymptotiques dâ€™un algorithme. Par asymptotique, on parle des grandes valeurs de lâ€™instance dâ€™entrÃ©e qui tendent vers lâ€™infini. Ce modÃ¨le permettrait alors de comparer les algorithmes qui rÃ©solvent un mÃªme problÃ¨me.

En dâ€™autres mots, câ€™est un outil qui va nous permettre dâ€™estimer le temps que prendra lâ€™algorithme Ã  exÃ©cuter toutes les instructions, on parlera de la complexitÃ© temporelle. Dâ€™un autre cÃ´tÃ©, cet outil va permettre dâ€™estimer lâ€™espace mÃ©moire nÃ©cessaire que prendra lâ€™algorithme pour exÃ©cuter toutes les instructions, on parlera de la complexitÃ© spatiale.

En parlant dâ€™instruction ou d'opÃ©ration Ã©lÃ©mentaire, pour illustrer ces concepts, prenons quelques exemples dâ€™algorithmes, dont la multiplication des entiers et la multiplication des matrices. Les opÃ©rations Ã©lÃ©mentaires Ã  considÃ©rer (affectation, incrÃ©mentation, comparaison, etc.) ne seront pas les mÃªmes. Dans le premier cas, on pourrait prendre la multiplication des bits (suite de 0 et 1) et dans le second cas la multiplication des entiers en base 10. Pour aller plus loin, prenons un algorithme de NLP, dans ce cas, lâ€™opÃ©ration Ã©lÃ©mentaire pourrait Ãªtre la multiplication des vecteurs reprÃ©sentant les mots. Pour un dernier exemple, lâ€™algorithme de tri, lâ€™opÃ©ration Ã©lÃ©mentaire serait le nombre de comparaisons.

> Petit exemple parlant, prenons le cas des LLM (Large Language Model), leurs complexitÃ©s temporelles et spatiales ne leur permettent pas de fonctionner correctement sur un pc classique, puisqu'on manquera dâ€™espace mÃ©moire nÃ©cessaire pour charger tout le modÃ¨le en mÃ©moire, et mÃªme si on arrive Ã  charger le modÃ¨le, mÃªme pour les petits modÃ¨les, la gÃ©nÃ©ration du texte prendra plusieurs minutes, ce qui les rendrait impraticables. (Connaissez-vous les complexitÃ©s du modÃ¨le GPT-3.5 ?) Les opÃ©rations Ã©lÃ©mentaires considÃ©rÃ©es ?)
> 

<aside>
ğŸ’¡ Par abus de langage, lorsqu'on parle de la complexitÃ© tout court dans la littÃ©rature, les blogs, etc., on fait rÃ©fÃ©rence Ã  la complexitÃ© temporelle.

</aside>

Donc, par complexitÃ©, on entend une fonction qui va compter le nombre des opÃ©rations Ã©lÃ©mentaires de notre algorithme lorsque lâ€™instance dâ€™entrÃ©e est trÃ¨s grande.

Il existe trois modÃ¨les mathÃ©matiques de complexitÃ© : 

1.   $\Omega$  est utilisÃ© afin dâ€™exprimer la complexitÃ© dans le meilleur des cas. Intuitivement, ce modÃ¨le va nous dire ceciÂ : pour nâ€™importe quelle instance du problÃ¨me, nous ne pourrons pas faire mieux (moins dâ€™opÃ©rations) que la borne dÃ©finie. Il dÃ©finit la borne infÃ©rieure asymptotique de la complexitÃ© de notre algorithme.
2. $O$  est utilisÃ© pour exprimer la complexitÃ© dans le pire des cas. Contrairement Ã   $\Omega$, ce modÃ¨le va nous dire ceciÂ : pour nâ€™importe quelle instance du problÃ¨me, nous ne pourrons pas faire pire que la borne dÃ©finie. Il dÃ©finit la borne supÃ©rieure asymptotique de la complexitÃ© de notre algorithme.
3.  $\Theta$ est utilisÃ© pour exprimer la complexitÃ© dans le pire et le meilleur cas. Pour nâ€™importe quelle instance, le modÃ¨le indiquera que la complexitÃ© de  lâ€™algorithme est majorÃ©e et minorÃ©e par une mÃªme fonction g Ã  deux constantes multiplicatives. Dans le meilleur des cas, ta complexitÃ© ne sera pas meilleure quâ€™une fonction g et dans le pire des cas, elle ne sera pas pire que la fonction g Ã  deux constantes multiplicatives. Ta complexitÃ© est prise en sandwich par la fonction g Ã  deux constantes multiplicatives. 

Ce qui est le plus intÃ©ressant, câ€™est la complexitÃ© dans le pire des cas, puisque si nous avons une bonne complexitÃ© dans le pire des cas, lâ€™algorithme ne peut quâ€™Ãªtre meilleur dans le meilleur des cas ou en moyenne (certains algorithmes sont meilleurs que dâ€™autres que dans le meilleur des cas). 

<aside>
ğŸ’¡ **Par abus de langage, complexitÃ©=complexitÃ© temporelle dans le pire des cas**

</aside>

> En tant que Machine Learning Engineer (MLE) ou Data Scientist (DS), il est important de penser Ã  la complexitÃ© de lâ€™algorithme quâ€™on met en place, quâ€™il sâ€™agisse dâ€™une architecture dâ€™un modÃ¨le, ou dâ€™un algorithme dâ€™entrainement ou dâ€™infÃ©rence.
> 

### **Formulation mathÃ©matique de la complÃ©xitÃ©**

Soient $g$ et $f$ deux fonction de $\N$ dans $\R$, 

**ComplexitÃ© dans le meilleur cas** 

$$
f \in \Omega(g) = \exists c>0,\ \exists N \in \N ,\ \forall n \ge N ,\ 0\le c g(n) \le f(n)    
$$

$f \in \Omega(g)$ ou $f = \Omega(g)$,   si et seulement si Ã  partir dâ€™un certain rang la fonction $f$ croÃ®t plus rapidement que la fonction $g$ Ã  une constante multiplicative prÃ¨s $c$. La fonction $cg(n)$  dÃ©finit une borne infÃ©rieure de la fonction $f$ Ã  partir du rang N. 

Dans le cadre de lâ€™algorithmie, $\Omega$  va nous informer que pour des instances de plus en plus grandes pour un algorithme telle que sa complexitÃ© est exprimÃ©e par la fonction $f$ , $f$ ne pourra pas croitre plus vite que la fonction  $g$. En pratique, ce modÃ¨le, cette notation est utilisÃ©e pour obtenir une borne infÃ©rieure dans le meilleur des cas. Il sâ€™agit aussi dâ€™une borne infÃ©rieure pour nâ€™importe quel autre cas.

**ComplexitÃ© dans le pire cas** 

$$
f \in O(g) = \exists c>0,\ \exists N \in \N ,\ \forall n \ge N ,\ 0\le f(n) \le c g(n)
$$

$f \in O(g)$ ou $f = O(g)$, si et seulement si Ã  partir dâ€™un certain rang la fonction $g$ croÃ®t plus rapidement que la fonction $f$ Ã  une constante multiplicative prÃ¨s $c$. La fonction $cg(n)$  dÃ©finit une borne supÃ©rieure de la fonction $f$ Ã  partir du rang N. 

Dans le cadre de lâ€™algorithmie, $\Omega$  va nous informer que pour des instances de plus en plus grandes, pour un algorithme dont sa complexitÃ© est exprimÃ©e par la fonction $f$ , $f$ ne pourra pas croitre plus vite que la fonction  $g$. En pratique, ce modÃ¨le, cette notation est utilisÃ©e pour obtenir une borne supÃ©rieure dans le pire des cas. Il sâ€™agit aussi dâ€™une borne supÃ©rieure pour nâ€™importe quel autre cas.

**ComplexitÃ© dans le pire et meilleur cas** 

$$
f \in \Theta(g) = \exists c,d>0,\  \exists N \in \N ,\ \forall n \ge N ,\ d g(n)\le f(n) \le c g(n)
$$

$f \in \Theta(g)$ ou $f = \Theta(g)$, si et seulement si Ã  partir dâ€™un certain rang les deux fonctions $f$ et $g$ ont la mÃªme croissance asymptotiquement, multipliÃ©e par deux constantes. La fonction $f$ est prise en sandwich par la fonction $g$ Ã  deux constantes multiplicatives prÃ¨s. En dâ€™autres mots, $f$ est majorÃ©e et minorÃ©e par $g$ Ã  deux constantes multiplicatives prÃ¨s, Ã  partir du rang N.

En algorithmique, $\Theta$ permet dâ€™exprimer une complexitÃ© dans le pire et le meilleur des cas. Pour nâ€™importe quelle instance du problÃ¨me, dans tous les cas, la complexitÃ© est minorÃ©e et majorÃ©e par $g$ Ã  deux constantes multiplicatives prÃ¨s.

### Exemple thÃ©orique

Soient deux fonctions $f=x^3$ et $g=\dfrac{1}{200}x^4$ positives dÃ©finies sur $\R$. 

![Untitled](/img/posts/complexity/f_g_functions.png)

A partir du $n=200$, la fonction $g$ croÃ®t plus rapidement que la fonction $f$, et inversement.  On peut affirmer que $f$ est en $O(g)$ et $g$ est en $\Omega(f)$ pour $N=200$, et $c=1$. 

### Exemple dâ€™un cas pratique dâ€™analyse dâ€™algorithme

1. **Rechercher dâ€™un Ã©lÃ©ment dans un tableau**

Nous avons un tableau contenant les Ã©lÃ©ments triÃ©s et nous souhaitons savoir si lâ€™Ã©lÃ©ment est prÃ©sent dans le tableau. En entrÃ©e, un tableau triÃ© et un Ã©lÃ©ment Ã  rechercher dans ce tableau. Il sâ€™agit dâ€™un problÃ¨me classique, rechercher un Ã©lÃ©ment dans une base de donnÃ©es par exemple.

**1.1 Analyse dâ€™algorithme version 1**

```python
def recherche_v1(L:List[int],e:int):	
    
    for elt in L:
        if elt==e:
            return True
    return False
```

Comptons le nombre dâ€™opÃ©rations Ã©lÃ©mentaires de cet algorithme. On notera $f$ ****la fonction qui dÃ©termine le nombre dâ€™opÃ©rations de notre algorithme.

```markup
A chaque tour de boucle
    1 affectation ( la variable elt )
    1 comparaison
Et une seule instruction return.
```

Le meilleur des cas est celui oÃ¹ la valeur recherchÃ©e se trouve Ã  la premiÃ¨re position du tableau. On aura un seul tour de boucle, donc 2 opÃ©rations en tout.  $f= 3$. Donc, peu importe la taille de la liste, notre fonction nâ€™exÃ©cutera que 3 instructions. Elle est en $\Theta(1)$ dans le meilleur des cas, donc $O(1)$  et en $\Omega(1)$. Il suffit de choisir $g=c$, oÃ¹ $c$ est une constante.

Le pire des cas est celui oÃ¹ la valeur recherchÃ©e nâ€™est pas prÃ©sente dans la liste. Donc, on aura $n$ tour de boucle pour un tableau de taille $n$,  2  opÃ©rations qui sâ€™exÃ©cutent n fois et une opÃ©ration de return:  $2n + 1$.  Donc $f=2n+1$. La fonction f dÃ©pend donc de la taille du tableau, elle dÃ©pend donc de n. On Ã©crira  $f(n)=2n+1$.  Elle est en $\Theta(n)$ dans le meilleur des cas, donc $O(n)$  et en $\Omega(n)$. Pour prouver que $f$ est $\Theta(n)$, on pourrait choisir $g(n)=n$, $d=2$, $c=3$, $N=2$. 

Peu importe lâ€™entrÃ©e, la complexitÃ© est en $O(n)$ et en $\Omega(1)$. Dans la pratique, on sâ€™intÃ©resse au cas le plus dÃ©favorable, on cherche donc la borne supÃ©rieure de notre algorithme, on notera $O(n)$ tout simplement comme Ã©tant la complexitÃ© de lâ€™algorithme. Notez quâ€™on ne pourra pas dire que cet algorithme est pas en $\Theta(n)$ (Pourquoi ? rÃ©ponse laissÃ©e au lecteur).

**1.2 Algorithme version 2 ( version rÃ©cursive )**

```python
def recherche_v2(L:List[int],e:int):
    
    res = len(L)
    if res==1:
        return L[0]==e
    
    if L[res//2]==e:
        return True
    
    if L[res//2]>e:
        return recherche_v2(L[:res//2],e)
    else:
        return recherche_v2(L[res//2:],e)
```

```markup
A chaque appel de fonction
    1 affectation ( la variable elt )
    3 comparaison
    une instruction return.
```

Le meilleur des cas est celui oÃ¹ lâ€™Ã©lÃ©ment recherchÃ© se trouve Ã  la premiÃ¨re position du tableau, donc on nâ€™aura quâ€™un seul appel de la fonction, et deux opÃ©rations Ã©lÃ©mentaires. On a une complexitÃ© constante  $\Theta(1)$ dans le meilleur des cas.

Dans le pire des cas, on a 4 opÃ©rations Ã  chaque appel. Combien y a-t-il dâ€™appels de fonction ? Ã€ chaque appel, la taille est divisÃ©e par 2. Si $n = [4,5,6,7]$ on a 2 appels, si $n = [8,...,15]$ on a trois appels, etc. Donc pour une liste de taille $n$ on a $\lfloor log (n) \rfloor$ appels.

Pour un tableau de taille $n$ on a $4*\lfloor log (n) \rfloor$  opÃ©rations Ã©lÃ©mentaires. On notera $f(n) = 4*\lfloor log (n) \rfloor$, la complexitÃ© de notre algorithme est de  $\Theta(logn)$.

Avec la mÃªme analyse faite pour lâ€™algorithme pour la version 1, avec la complexitÃ© dans le meilleur des cas $\Theta(1)$  et dans le pire des cas $\Theta(logn)$, on peut conclure que la complexitÃ© de cet algorithme version 2 est  $O(logn)$.

1. **$x^n \mod y$**

On souhaite calculer  **x puissance n modulo y.** Ce type de calcul est trÃ¨s utilisÃ© dans la cryptographie, la thÃ©orie des nombres, etc. On va proposer deux versions dâ€™algorithmes et ensuite Ã©valuer leurs complexitÃ©s

**2.1 Analyse version 1**

```python
def puissance_v1(x:int,n:int):
    
    if n==0: return 1
    
    res = x
    for i in range(n-1):
        res=res*x
        res=res%27
    return res
```

```markup
1 comparaison
1 instruction return
A chaque tour de boucle
    3 affectation ( les variable i et res)
    1 multiplication
    1 divsion euclidienne (modulo) 
une instruction return.
```

La boucle est exÃ©cutÃ©e n fois, donc on a $3 + 5n$ instructions au total. Dans le meilleur comme dans le pire des cas, lâ€™algorithme exÃ©cutera n tours de boucle pour une valeur n, donc la complexitÃ© est en $O(n)$.

**2.2 Analyse version 2**

```python
def puissance_v2(x:int,n:int):
    
    if n==0: return 0
    if n==1: return 1
    
    tmp = puissance_v2(x,n//2)
    if n%2==0: 
        return tmps*tmps%27
    else:
        return tmps*tmps*x%27
```

```markup
A chaque appel de fonction
    2 comparaisons
    (1 ou 2 instructions return, pour n=1,2)
    1 affectation ( la variable tmp)
    1 test
    2 divisions euclidiennes
    2 ou 3 multiplication
    1 instruction return.
```

Ã€ chaque appel de fonction, nous avons 9 instructions Ã©lÃ©mentaires, Ã  1 ou 2 prÃ¨s. Combien dâ€™appels de fonction sont faits ? Il y a $\lfloor log (n) \rfloor$ appels, puisquâ€™Ã  chaque appel $n$ est divisÃ© par deux pour lâ€™appel suivant. Au total on a $9*\log(n)$ opÃ©rations Ã©lÃ©mentaires. La complexitÃ© est en $O(\log(n))$.

1. **Suite de Fibonacci (**Exercice laissÃ© aux lecteurs motivÃ©s.**)**

Voici deux implÃ©mentations diffÃ©rentes de la suite de Fibonacci, analysez leurs complexitÃ©s et comparez-les. 

```python
def fibonacci_v1(n:int):
    
    if n==0: return 0
    res1=0
    res2=1
    
    for i in range(1,n):
        tmp = res1+res2
        res1 = res2
        res2 = tmp
    return res2
```

```python
def fibonacci_v2(n:int):
    
    if n==0: return 0
    if n==1: return 1
    
    res1= fibonacci_v2(n-1)
    res2= fibonacci_v2(n-2)
    return res1 + res2
```

### ComplexitÃ© dans le Machine Learning

Nous allons dans un premier temps Ã©tudier la complexitÃ© des deux algorithmes de base en Machine Learning que nous connaissons bien, du moins pour les initiÃ©s et les avertis. Pour mieux les comprendre, nous allons les recoder Ã  partir de zÃ©ro en utilisant la librairie numpy, ensuite estimer leurs complexitÃ©s pour lâ€™entrainement et lâ€™infÃ©rence.

Supposons que la matrice $X$ reprÃ©sentant le jeu de donnÃ©es, elle est de taille $n*p$ oÃ¹ $n$ est le nombre dâ€™observations dans le jeu de donnÃ©es et $p$ le nombre de caractÃ©ristiques (features).  Notons  $n>>p$, ce qui est gÃ©nÃ©ralement le cas pour les jeux de donnÃ©es oÃ¹ la dimension des Ã©chantillons peut Ãªtre nÃ©gligeable par rapport Ã  la taille du jeu de donnÃ©es. Les opÃ©rations Ã©lÃ©mentaires ici seront les multiplications des entiers et les allocations mÃ©moires pour les variables pour nos calculs de complexitÃ©s. La matrice $y$ est de taille $n$ Ã©galement et reprÃ©sente les Ã©tiquettes pour chaque donnÃ©e (ligne de la matrice $X$).

1. **Cas dâ€™une rÃ©gression LinÃ©aire ( Linear Regression)**

```python
import numpy as np

class LinearRegression():
    """ Linear regression model using normal equation """
    def __init__(self) -> None:
        self.weights = None

    def fit(self, X, y):
        if self.weights is not None:
            raise ValueError("Model is already fit")
        
        X = np.hstack((np.ones((X.shape[0], 1)), X))
        invers_X = np.linalg.inv(X.T.dot(X))
        self.weights = invers_X.dot(X.T).dot(y)

    def predict(self, X):
        if self.weights is None:
            raise ValueError("Model is not fit yet")
        if self.weights.shape[0] != X.shape[1] + 1:
            raise ValueError("Input shape is not compatible with model")
        X = np.hstack((np.ones((X.shape[0], 1)), X))
        return self.weights.T.dot(X)
```

Analysons la complexitÃ© temporelle et spatiale de ce code:

**ComplexitÃ© temporelle**

1. La mÃ©thode __**init**__

Cette mÃ©thode ne fait pratiquement rien en termes dâ€™opÃ©rations, sa complexitÃ© est constante $O(1)$.

1. La mÃ©thode **fit** (la mÃ©thode qui entraine le modÃ¨le)

On a une premiÃ¨re multiplication $X'\times X$, pour chaque Ã©lÃ©ment de la nouvelle matrice il faut $p$ multiplications et $p-1$ additions. Pour donc la nouvelle matrice qui contiendra $np$ Ã©lÃ©ments, il faut $(p + (p-1))\times np \approx 2p^2n$, donc la complexitÃ© est **$O(p^2n)$.**    

Lâ€™inverse matriciel avec np.linalg.inv de la bibliothÃ¨que numpy a une complexitÃ© $O(n^3)$ pour une matrice carrÃ© de taille $n$. Je laisserai au lecteur le soins de vÃ©rifier cette complexitÃ©.   

Ensuite, on a une multiplication dâ€™une matrice et dâ€™un vecteur. Comme pour la premiÃ¨re multiplication, pour chaque Ã©lÃ©ment on a $2p-1$ opÃ©rations, donc au total $p(2p-1)$, la complexitÃ© est donc $O(p^2)$.

En faisant la somme de ces diffÃ©rentes complexitÃ©s, la complexitÃ© est de $O(p^2 + p^2n + p^3) = O(p^2n + p^3)$.

1. La mÃ©thode **predict** ( mÃ©thode dâ€™infÃ©rence)

Dans cette mÃ©thode, on a une seule multiplication entre le vecteur dâ€™une taille $p$ contenant les poids et une matrice de taille $np$ contenant les features pour chaque exemple. Pour calculer la valeur finale, on aura $nÂ²p$ opÃ©rations, la complexitÃ© est $O(pÂ²n)$.

**ComplexitÃ© spatiale**

Dans la mÃ©thode init on ne stocke rien, si ce nâ€™est la rÃ©fÃ©rence None Ã  la variable. On a une complexitÃ© constante $O(1)$. Dans la mÃ©thode fit dans un premier temps, on stocke la matrice X de taille $n \times p$  reprÃ©sentant le jeu de donnÃ©es Ã  laquelle on rajoute un vecteur unitaire, donc on a une allocation mÃ©moire de $(n \times p )+ p$. La variable invers_X est de meme taille et occupe donc un espace mÃ©moire de  $n \times p$. La variable weight est un vecteur de taille $p$ et occupe donc un espace mÃ©moire, la complÃ©xitÃ© est alors $O(np + p)= O(np)$.

1. **Cas de la rÃ©gression Logistique (Logistic Regression)**

```python
import numpy as np

class logistic_regression():

    def __init__(self, lr:int=0.001, n_iter:int=1000,epsilon=1e-9, treshold=.5):
        """ initialise all parameters """
        
        self.lr = lr
        self.n_iter = n_iter
        self.epsilon=epsilon
        self.treshold=treshold
        self.print_loss=True

        # model parameters
        self.w = None
        self.b = None
        
    def _sigmoid_function(self, x:np.array)-> np.array:
        """ Compute the sigmoid value of a vector x """
        
        return 1/(1+np.exp(-x))

    def feed_forward(self,X:np.array) -> np.array:
        """Compute the ouput probability"""

        z = X@self.w + self.b
        return self._sigmoid_function(z)

    def compute_loss(self, y_true:np.array, y_pred:np.array):
        """ Compute the binary cross entropy """
        
        #epsilon if y_pred equal zero or one
        res1 = y_true*np.log(y_pred+self.epsilon)
        res2 = (1-y_true)*np.log(1-y_pred+self.epsilon)        
        
        return -np.mean(res1+res2)

    def fit(self, X:np.array, y:np.array) -> None:
        """ Fit the model weight using gradient descent algorithm """

        self.w, self.b = np.zeros(X.shape[1]), 0

        for i in range(self.n_iter):

            output_prob = self.feed_forward(X)
            dz =  output_prob - y
            dw = np.mean(X.T@dz)
            db = np.mean(dz)

            self.w = self.w - self.lr*dw
            self.b = self.b - self.lr*db

    def predict(self,X:np.array)-> np.array:
        
        if self.w is None or self.b is None:
            raise('Model has not been fitted yet, please run fit first !')
        output_prob = self.feed_forward(X)
        output_lab = [1 if pred>self.treshold else 0 for pred in output_prob]

        return output_lab

```

**ComplexitÃ© temporelle**

La mÃ©thode init a une complexitÃ© temporelle constante, elle ne stocke que des constantes indÃ©pendantes des entrÃ©es et nâ€™effectue aucun calcul. 

La mÃ©thode sigmoid effectue une opÃ©ration dâ€™exp, en considÃ©rant cette opÃ©ration Ã©lÃ©mentaire pour chaque Ã©lÃ©ment du vecteur de taille $p$, on a donc au total $p$ opÃ©rations, donc une complexitÃ© en $O(p)$. 

La mÃ©thode feed_forward fait une multiplication matricielle dont la complÃ©xitÃ© est $O(pÂ²n)$, une addition matricielle dont la complÃ©xitÃ© est $O(p)$, et enfin la mÃ©thode sigmoid dont la complexite est $O(p)$. En addition ces complÃ©xitÃ© est en tenant compte de lâ€™ordre de grandeur; cette mÃ©thode a une complÃ©xitÃ© $O(pÂ²n)$.  

La mÃ©thode compute_loss prend en entrÃ©e deux vecteurs de mÃªme taille $p$. Il y a quatre opÃ©rations  addition/soustraction, et deux multiplications vectorielles (Ã©lÃ©ment par Ã©lÃ©ment) entre vecteurs dont la complexitÃ© est $O(p)$. Lâ€™opÃ©ration np.log est constante, sur un vecteur de taille p, sa complexitÃ© est $O(p)$. En additionnant ces complexitÃ©s, on obtient une complexitÃ© $O(p)$. 

La mÃ©thode fit, la premiÃ¨re ligne est une affectation pas de calcul. Examinons un tour de boucle :

La premiÃ¨re instruction est un appel Ã  feed_forward dont la complexitÃ© est en $O(pÂ²n)$. Ensuite, une multiplication matricielle (matrice-vecteur) dont la complexitÃ© est la mÃªme que la mÃ©thode feed_forward. La fonction np.mean de numpy a une complexitÃ© linÃ©aire $O(p)$, les autres opÃ©rations dâ€™additions et multiplications sont constantes. Pour la mise Ã  jour de w, on a $O(p)$ en termes de complexitÃ©. En additionnant, on obtient une complexitÃ© $O(pÂ²n)$. 

Pour un nombre $m$ dâ€™itÃ©rations, on aura $m*O(pÂ²n)$, et donc la complexitÃ© de cette mÃ©thode fit est de $O(mnpÂ²)$. 

Pour la mÃ©thode predict, on a un appel Ã  feed_forward et une boucle sur chaque Ã©lÃ©ment du vecteur de probabilitÃ© de taille $p$, donc cette mÃ©thode a une complexitÃ© $O(pÂ²n)$. 

**ComplÃ©xitÃ© spatiale**

La mÃ©thode **init** a une complexitÃ© constante, elle ne stocke que des constantes. La mÃ©thode _sigmoid_function a besoin de stocker le rÃ©sultat, dont un vecteur de taille $p$. La complexitÃ© est en $O(p)$. Pour la mÃ©thode feed_forward, il faut stocker le vecteur z de taille $p$, et le rÃ©sultat qui est de taille $p$, donc $2p$, la complexitÃ© est de $O(p)$. La mÃ©thode compute_loss stocke les deux variables res1 et res2, ainsi que le rÃ©sultat, chacune a besoin dâ€™une allocation mÃ©moire de $p$, donc une complexitÃ© de $O(p)$. 

La mÃ©thode **fit** avant ****la boucle ne stocke que deux variables dont lâ€™espace mÃ©moire est de taille p. Les six variables dans la boucle ont besoin chacune dâ€™un espace mÃ©moire de p, donc la complexitÃ© pour un tour de boucle est $O(p)$, donc pour m tour de boucle, on aura une complexitÃ© finale $O(mp)$. Easy !

**Conclusion**

Lâ€™Ã©tude de la complexitÃ© ne peut Ãªtre rÃ©duite Ã  ce blog. Le but ici Ã©tait de vous donner quelques Ã©lÃ©ments clÃ©s pour Ã©valuer la complexitÃ© dâ€™un algorithme de machine learning ou non. La complexitÃ© ne remplace pas le benchmark, mais câ€™est un outil qui permet dâ€™avoir une idÃ©e du comportement dâ€™un algorithme indÃ©pendamment du langage dâ€™implÃ©mentation et des dÃ©tails dâ€™optimisation liÃ©s Ã  ce langage. La constante que cache la notation  peut Ãªtre trompeuse, puisquâ€™elle peut Ãªtre trÃ¨s grande et quâ€™en pratique, un algorithme de complexitÃ© polynomiale soit plus rapide quâ€™un algorithme de complexitÃ© logarithmique. Le Quick sort nâ€™est pas le meilleur algorithme de tri en ce qui concerne la complexitÃ©, mais il est tout de mÃªme le meilleur en pratique et reste un choix par dÃ©faut.