---
layout: post
title: "Complexit√©"
subtitle: "introduction √† la notion de complexit√© d'algorithme."
date: 2020-01-26 23:45:13 -0400
background: '/img/posts/complexity/complexity.png'
---

# Introduction sur complexit√© algorithmique

### Qu‚Äôest-ce qu‚Äôune complexit√© algorithmique ?

C‚Äôest un outil ou mod√®le math√©matique important en science informatique qui permet d‚Äôestimer les performances asymptotiques d‚Äôun algorithme. Par asymptotique, on parle des grandes valeurs de l‚Äôinstance d‚Äôentr√©e qui tendent vers l‚Äôinfini. Ce mod√®le permettrait alors de comparer les algorithmes qui r√©solvent un m√™me probl√®me.

En d‚Äôautres mots, c‚Äôest un outil qui va nous permettre d‚Äôestimer le temps que prendra l‚Äôalgorithme √† ex√©cuter toutes les instructions, on parlera de la complexit√© temporelle. D‚Äôun autre c√¥t√©, cet outil va permettre d‚Äôestimer l‚Äôespace m√©moire n√©cessaire que prendra l‚Äôalgorithme pour ex√©cuter toutes les instructions, on parlera de la complexit√© spatiale.

En parlant d‚Äôinstruction ou d'op√©ration √©l√©mentaire, pour illustrer ces concepts, prenons quelques exemples d‚Äôalgorithmes, dont la multiplication des entiers et la multiplication des matrices. Les op√©rations √©l√©mentaires √† consid√©rer (affectation, incr√©mentation, comparaison, etc.) ne seront pas les m√™mes. Dans le premier cas, on pourrait prendre la multiplication des bits (suite de 0 et 1) et dans le second cas la multiplication des entiers en base 10. Pour aller plus loin, prenons un algorithme de NLP, dans ce cas, l‚Äôop√©ration √©l√©mentaire pourrait √™tre la multiplication des vecteurs repr√©sentant les mots. Pour un dernier exemple, l‚Äôalgorithme de tri, l‚Äôop√©ration √©l√©mentaire serait le nombre de comparaisons.

> Petit exemple parlant, prenons le cas des LLM (Large Language Model), leurs complexit√©s temporelles et spatiales ne leur permettent pas de fonctionner correctement sur un pc classique, puisqu'on manquera d‚Äôespace m√©moire n√©cessaire pour charger tout le mod√®le en m√©moire, et m√™me si on arrive √† charger le mod√®le, m√™me pour les petits mod√®les, la g√©n√©ration du texte prendra plusieurs minutes, ce qui les rendrait impraticables. (Connaissez-vous les complexit√©s du mod√®le GPT-3.5 ?) Les op√©rations √©l√©mentaires consid√©r√©es ?)
> 

<aside>
üí° Par abus de langage, lorsqu'on parle de la complexit√© tout court dans la litt√©rature, les blogs, etc., on fait r√©f√©rence √† la complexit√© temporelle.

</aside>

Donc, par complexit√©, on entend une fonction qui va compter le nombre des op√©rations √©l√©mentaires de notre algorithme lorsque l‚Äôinstance d‚Äôentr√©e est tr√®s grande.

Il existe trois mod√®les math√©matiques de complexit√© : 

1.   $\Omega$  est utilis√© afin d‚Äôexprimer la complexit√© dans le meilleur des cas. Intuitivement, ce mod√®le va nous dire ceci¬†: pour n‚Äôimporte quelle instance du probl√®me, nous ne pourrons pas faire mieux (moins d‚Äôop√©rations) que la borne d√©finie. Il d√©finit la borne inf√©rieure asymptotique de la complexit√© de notre algorithme.
2. $O$  est utilis√© pour exprimer la complexit√© dans le pire des cas. Contrairement √†  $\Omega$, ce mod√®le va nous dire ceci¬†: pour n‚Äôimporte quelle instance du probl√®me, nous ne pourrons pas faire pire que la borne d√©finie. Il d√©finit la borne sup√©rieure asymptotique de la complexit√© de notre algorithme.
3.  $\Theta$ est utilis√© pour exprimer la complexit√© dans le pire et le meilleur cas. Pour n‚Äôimporte quelle instance, le mod√®le indiquera que la complexit√© de  l‚Äôalgorithme est major√©e et minor√©e par une m√™me fonction g √† deux constantes multiplicatives. Dans le meilleur des cas, ta complexit√© ne sera pas meilleure qu‚Äôune fonction g et dans le pire des cas, elle ne sera pas pire que la fonction g √† deux constantes multiplicatives. Ta complexit√© est prise en sandwich par la fonction g √† deux constantes multiplicatives. 

Ce qui est le plus int√©ressant, c‚Äôest la complexit√© dans le pire des cas, puisque si nous avons une bonne complexit√© dans le pire des cas, l‚Äôalgorithme ne peut qu‚Äô√™tre meilleur dans le meilleur des cas ou en moyenne (certains algorithmes sont meilleurs que d‚Äôautres que dans le meilleur des cas). 

<aside>
üí° **Par abus de langage, complexit√©=complexit√© temporelle dans le pire des cas**

</aside>

> En tant que Machine Learning Engineer (MLE) ou Data Scientist (DS), il est important de penser √† la complexit√© de l‚Äôalgorithme qu‚Äôon met en place, qu‚Äôil s‚Äôagisse d‚Äôune architecture d‚Äôun mod√®le, ou d‚Äôun algorithme d‚Äôentrainement ou d‚Äôinf√©rence.
> 

### **Formulation math√©matique de la compl√©xit√©**

Soient $g$ et $f$ deux fonction de $\N$ dans $\R$, 

**Complexit√© dans le meilleur cas** 

$$
f \in \Omega(g) = \exists c>0,\ \exists N \in \N ,\ \forall n \ge N ,\ 0\le c g(n) \le f(n)    
$$

$f \in \Omega(g)$ ou $f = \Omega(g)$,   si et seulement si √† partir d‚Äôun certain rang la fonction $f$ cro√Æt plus rapidement que la fonction $g$ √† une constante multiplicative pr√®s $c$. La fonction $cg(n)$  d√©finit une borne inf√©rieure de la fonction $f$ √† partir du rang N. 

Dans le cadre de l‚Äôalgorithmie, $\Omega$  va nous informer que pour des instances de plus en plus grandes pour un algorithme telle que sa complexit√© est exprim√©e par la fonction $f$ , $f$ ne pourra pas croitre plus vite que la fonction  $g$. En pratique, ce mod√®le, cette notation est utilis√©e pour obtenir une borne inf√©rieure dans le meilleur des cas. Il s‚Äôagit aussi d‚Äôune borne inf√©rieure pour n‚Äôimporte quel autre cas.

**Complexit√© dans le pire cas** 

$$
f \in O(g) = \exists c>0,\ \exists N \in \N ,\ \forall n \ge N ,\ 0\le f(n) \le c g(n)
$$

$f \in O(g)$ ou $f = O(g)$, si et seulement si √† partir d‚Äôun certain rang la fonction $g$ cro√Æt plus rapidement que la fonction $f$ √† une constante multiplicative pr√®s $c$. La fonction $cg(n)$  d√©finit une borne sup√©rieure de la fonction $f$ √† partir du rang N. 

Dans le cadre de l‚Äôalgorithmie, $\Omega$  va nous informer que pour des instances de plus en plus grandes, pour un algorithme dont sa complexit√© est exprim√©e par la fonction $f$ , $f$ ne pourra pas croitre plus vite que la fonction  $g$. En pratique, ce mod√®le, cette notation est utilis√©e pour obtenir une borne sup√©rieure dans le pire des cas. Il s‚Äôagit aussi d‚Äôune borne sup√©rieure pour n‚Äôimporte quel autre cas.

**Complexit√© dans le pire et meilleur cas** 

$$
f \in \Theta(g) = \exists c,d>0,\  \exists N \in \N ,\ \forall n \ge N ,\ d g(n)\le f(n) \le c g(n)
$$

$f \in \Theta(g)$ ou $f = \Theta(g)$, si et seulement si √† partir d‚Äôun certain rang les deux fonctions $f$ et $g$ ont la m√™me croissance asymptotiquement, multipli√©e par deux constantes. La fonction $f$ est prise en sandwich par la fonction $g$ √† deux constantes multiplicatives pr√®s. En d‚Äôautres mots, $f$ est major√©e et minor√©e par $g$ √† deux constantes multiplicatives pr√®s, √† partir du rang N.

En algorithmique, $\Theta$ permet d‚Äôexprimer une complexit√© dans le pire et le meilleur des cas. Pour n‚Äôimporte quelle instance du probl√®me, dans tous les cas, la complexit√© est minor√©e et major√©e par $g$ √† deux constantes multiplicatives pr√®s.

### Exemple th√©orique

Soient deux fonctions $f=x^3$ et $g=\dfrac{1}{200}x^4$ positives d√©finies sur $\R$. 

![Untitled](/img/posts/complexity/f_g_functions.png)

A partir du $n=200$, la fonction $g$ cro√Æt plus rapidement que la fonction $f$, et inversement.  On peut affirmer que $f$ est en $O(g)$ et $g$ est en $\Omega(f)$ pour $N=200$, et $c=1$. 

### Exemple d‚Äôun cas pratique d‚Äôanalyse d‚Äôalgorithme

1. **Rechercher d‚Äôun √©l√©ment dans un tableau**

Nous avons un tableau contenant les √©l√©ments tri√©s et nous souhaitons savoir si l‚Äô√©l√©ment est pr√©sent dans le tableau. En entr√©e, un tableau tri√© et un √©l√©ment √† rechercher dans ce tableau. Il s‚Äôagit d‚Äôun probl√®me classique, rechercher un √©l√©ment dans une base de donn√©es par exemple.

**1.1 Analyse d‚Äôalgorithme version 1**

```python
def recherche_v1(L:List[int],e:int):	
    
    for elt in L:
        if elt==e:
            return True
    return False
```

Comptons le nombre d‚Äôop√©rations √©l√©mentaires de cet algorithme. On notera $f$ ****la fonction qui d√©termine le nombre d‚Äôop√©rations de notre algorithme.

```markup
A chaque tour de boucle
    1 affectation ( la variable elt )
    1 comparaison
Et une seule instruction return.
```

Le meilleur des cas est celui o√π la valeur recherch√©e se trouve √† la premi√®re position du tableau. On aura un seul tour de boucle, donc 2 op√©rations en tout.  $f= 3$. Donc, peu importe la taille de la liste, notre fonction n‚Äôex√©cutera que 3 instructions. Elle est en $\Theta(1)$ dans le meilleur des cas, donc $O(1)$  et en $\Omega(1)$. Il suffit de choisir $g=c$, o√π $c$ est une constante.

Le pire des cas est celui o√π la valeur recherch√©e n‚Äôest pas pr√©sente dans la liste. Donc, on aura $n$ tour de boucle pour un tableau de taille $n$,  2  op√©rations qui s‚Äôex√©cutent n fois et une op√©ration de return:  $2n + 1$.  Donc $f=2n+1$. La fonction f d√©pend donc de la taille du tableau, elle d√©pend donc de n. On √©crira  $f(n)=2n+1$.  Elle est en $\Theta(n)$ dans le meilleur des cas, donc $O(n)$  et en $\Omega(n)$. Pour prouver que $f$ est $\Theta(n)$, on pourrait choisir $g(n)=n$, $d=2$, $c=3$, $N=2$. 

Peu importe l‚Äôentr√©e, la complexit√© est en $O(n)$ et en $\Omega(1)$. Dans la pratique, on s‚Äôint√©resse au cas le plus d√©favorable, on cherche donc la borne sup√©rieure de notre algorithme, on notera $O(n)$ tout simplement comme √©tant la complexit√© de l‚Äôalgorithme. Notez qu‚Äôon ne pourra pas dire que cet algorithme est pas en $\Theta(n)$ (Pourquoi ? r√©ponse laiss√©e au lecteur).

**1.2 Algorithme version 2 ( version r√©cursive )**

```python
def recherche_v2(L:List[int],e:int):
    
    res = len(L)
    if res==1:
        return L[0]==e
    
    if L[res//2]==e:
        return True
    
    if L[res//2]>e:
        return recherche_v2(L[:res//2],e)
    else:
        return recherche_v2(L[res//2:],e)
```

```markup
A chaque appel de fonction
    1 affectation ( la variable elt )
    3 comparaison
    une instruction return.
```

Le meilleur des cas est celui o√π l‚Äô√©l√©ment recherch√© se trouve √† la premi√®re position du tableau, donc on n‚Äôaura qu‚Äôun seul appel de la fonction, et deux op√©rations √©l√©mentaires. On a une complexit√© constante  $\Theta(1)$ dans le meilleur des cas.

Dans le pire des cas, on a 4 op√©rations √† chaque appel. Combien y a-t-il d‚Äôappels de fonction ? √Ä chaque appel, la taille est divis√©e par 2. Si $n = [4,5,6,7]$ on a 2 appels, si $n = [8,...,15]$ on a trois appels, etc. Donc pour une liste de taille $n$ on a $\lfloor log (n) \rfloor$ appels.

Pour un tableau de taille $n$ on a $4*\lfloor log (n) \rfloor$  op√©rations √©l√©mentaires. On notera $f(n) = 4*\lfloor log (n) \rfloor$, la complexit√© de notre algorithme est de  $\Theta(logn)$.

Avec la m√™me analyse faite pour l‚Äôalgorithme pour la version 1, avec la complexit√© dans le meilleur des cas $\Theta(1)$  et dans le pire des cas $\Theta(logn)$, on peut conclure que la complexit√© de cet algorithme version 2 est  $O(logn)$.

1. **$x^n \mod y$**

On souhaite calculer  **x puissance n modulo y.** Ce type de calcul est tr√®s utilis√© dans la cryptographie, la th√©orie des nombres, etc. On va proposer deux versions d‚Äôalgorithmes et ensuite √©valuer leurs complexit√©s

**2.1 Analyse version 1**

```python
def puissance_v1(x:int,n:int):
    
    if n==0: return 1
    
    res = x
    for i in range(n-1):
        res=res*x
        res=res%27
    return res
```

```markup
1 comparaison
1 instruction return
A chaque tour de boucle
    3 affectation ( les variable i et res)
    1 multiplication
    1 divsion euclidienne (modulo) 
une instruction return.
```

La boucle est ex√©cut√©e n fois, donc on a $3 + 5n$ instructions au total. Dans le meilleur comme dans le pire des cas, l‚Äôalgorithme ex√©cutera n tours de boucle pour une valeur n, donc la complexit√© est en $O(n)$.

**2.2 Analyse version 2**

```python
def puissance_v2(x:int,n:int):
    
    if n==0: return 0
    if n==1: return 1
    
    tmp = puissance_v2(x,n//2)
    if n%2==0: 
        return tmps*tmps%27
    else:
        return tmps*tmps*x%27
```

```markup
A chaque appel de fonction
    2 comparaisons
    (1 ou 2 instructions return, pour n=1,2)
    1 affectation ( la variable tmp)
    1 test
    2 divisions euclidiennes
    2 ou 3 multiplication
    1 instruction return.
```

√Ä chaque appel de fonction, nous avons 9 instructions √©l√©mentaires, √† 1 ou 2 pr√®s. Combien d‚Äôappels de fonction sont faits ? Il y a $\lfloor log (n) \rfloor$ appels, puisqu‚Äô√† chaque appel $n$ est divis√© par deux pour l‚Äôappel suivant. Au total on a $9*\log(n)$ op√©rations √©l√©mentaires. La complexit√© est en $O(\log(n))$.

1. **Suite de Fibonacci (**Exercice laiss√© aux lecteurs motiv√©s.**)**

Voici deux impl√©mentations diff√©rentes de la suite de Fibonacci, analysez leurs complexit√©s et comparez-les. 

```python
def fibonacci_v1(n:int):
    
    if n==0: return 0
    res1=0
    res2=1
    
    for i in range(1,n):
        tmp = res1+res2
        res1 = res2
        res2 = tmp
    return res2
```

```python
def fibonacci_v2(n:int):
    
    if n==0: return 0
    if n==1: return 1
    
    res1= fibonacci_v2(n-1)
    res2= fibonacci_v2(n-2)
    return res1 + res2
```

### Complexit√© dans le Machine Learning

Nous allons dans un premier temps √©tudier la complexit√© des deux algorithmes de base en Machine Learning que nous connaissons bien, du moins pour les initi√©s et les avertis. Pour mieux les comprendre, nous allons les recoder √† partir de z√©ro en utilisant la librairie numpy, ensuite estimer leurs complexit√©s pour l‚Äôentrainement et l‚Äôinf√©rence.

Supposons que la matrice $X$ repr√©sentant le jeu de donn√©es, elle est de taille $n*p$ o√π $n$ est le nombre d‚Äôobservations dans le jeu de donn√©es et $p$ le nombre de caract√©ristiques (features).  Notons  $n>>p$, ce qui est g√©n√©ralement le cas pour les jeux de donn√©es o√π la dimension des √©chantillons peut √™tre n√©gligeable par rapport √† la taille du jeu de donn√©es. Les op√©rations √©l√©mentaires ici seront les multiplications des entiers et les allocations m√©moires pour les variables pour nos calculs de complexit√©s. La matrice $y$ est de taille $n$ √©galement et repr√©sente les √©tiquettes pour chaque donn√©e (ligne de la matrice $X$).

1. **Cas d‚Äôune r√©gression Lin√©aire ( Linear Regression)**

```python
import numpy as np

class LinearRegression():
    """ Linear regression model using normal equation """
    def __init__(self) -> None:
        self.weights = None

    def fit(self, X, y):
        if self.weights is not None:
            raise ValueError("Model is already fit")
        
        X = np.hstack((np.ones((X.shape[0], 1)), X))
        invers_X = np.linalg.inv(X.T.dot(X))
        self.weights = invers_X.dot(X.T).dot(y)

    def predict(self, X):
        if self.weights is None:
            raise ValueError("Model is not fit yet")
        if self.weights.shape[0] != X.shape[1] + 1:
            raise ValueError("Input shape is not compatible with model")
        X = np.hstack((np.ones((X.shape[0], 1)), X))
        return self.weights.T.dot(X)
```

Analysons la complexit√© temporelle et spatiale de ce code:

**Complexit√© temporelle**

1. La m√©thode __**init**__

Cette m√©thode ne fait pratiquement rien en termes d‚Äôop√©rations, sa complexit√© est constante $O(1)$.

1. La m√©thode **fit** (la m√©thode qui entraine le mod√®le)

On a une premi√®re multiplication $X'\times X$, pour chaque √©l√©ment de la nouvelle matrice il faut $p$ multiplications et $p-1$ additions. Pour donc la nouvelle matrice qui contiendra $np$ √©l√©ments, il faut $(p + (p-1))\times np \approx 2p^2n$, donc la complexit√© est **$O(p^2n)$.**    

L‚Äôinverse matriciel avec np.linalg.inv de la biblioth√®que numpy a une complexit√© $O(n^3)$ pour une matrice carr√© de taille $n$. Je laisserai au lecteur le soins de v√©rifier cette complexit√©.   

Ensuite, on a une multiplication d‚Äôune matrice et d‚Äôun vecteur. Comme pour la premi√®re multiplication, pour chaque √©l√©ment on a $2p-1$ op√©rations, donc au total $p(2p-1)$, la complexit√© est donc $O(p^2)$.

En faisant la somme de ces diff√©rentes complexit√©s, la complexit√© est de $O(p^2 + p^2n + p^3) = O(p^2n + p^3)$.

1. La m√©thode **predict** ( m√©thode d‚Äôinf√©rence)

Dans cette m√©thode, on a une seule multiplication entre le vecteur d‚Äôune taille $p$ contenant les poids et une matrice de taille $np$ contenant les features pour chaque exemple. Pour calculer la valeur finale, on aura $n¬≤p$ op√©rations, la complexit√© est $O(p¬≤n)$.

**Complexit√© spatiale**

Dans la m√©thode init on ne stocke rien, si ce n‚Äôest la r√©f√©rence None √† la variable. On a une complexit√© constante $O(1)$. Dans la m√©thode fit dans un premier temps, on stocke la matrice X de taille $n \times p$  repr√©sentant le jeu de donn√©es √† laquelle on rajoute un vecteur unitaire, donc on a une allocation m√©moire de $(n \times p )+ p$. La variable invers_X est de meme taille et occupe donc un espace m√©moire de  $n \times p$. La variable weight est un vecteur de taille $p$ et occupe donc un espace m√©moire, la compl√©xit√© est alors $O(np + p)= O(np)$.

1. **Cas de la r√©gression Logistique (Logistic Regression)**

```python
import numpy as np

class logistic_regression():

    def __init__(self, lr:int=0.001, n_iter:int=1000,epsilon=1e-9, treshold=.5):
        """ initialise all parameters """
        
        self.lr = lr
        self.n_iter = n_iter
        self.epsilon=epsilon
        self.treshold=treshold
        self.print_loss=True

        # model parameters
        self.w = None
        self.b = None
        
    def _sigmoid_function(self, x:np.array)-> np.array:
        """ Compute the sigmoid value of a vector x """
        
        return 1/(1+np.exp(-x))

    def feed_forward(self,X:np.array) -> np.array:
        """Compute the ouput probability"""

        z = X@self.w + self.b
        return self._sigmoid_function(z)

    def compute_loss(self, y_true:np.array, y_pred:np.array):
        """ Compute the binary cross entropy """
        
        #epsilon if y_pred equal zero or one
        res1 = y_true*np.log(y_pred+self.epsilon)
        res2 = (1-y_true)*np.log(1-y_pred+self.epsilon)        
        
        return -np.mean(res1+res2)

    def fit(self, X:np.array, y:np.array) -> None:
        """ Fit the model weight using gradient descent algorithm """

        self.w, self.b = np.zeros(X.shape[1]), 0

        for i in range(self.n_iter):

            output_prob = self.feed_forward(X)
            dz =  output_prob - y
            dw = np.mean(X.T@dz)
            db = np.mean(dz)

            self.w = self.w - self.lr*dw
            self.b = self.b - self.lr*db

    def predict(self,X:np.array)-> np.array:
        
        if self.w is None or self.b is None:
            raise('Model has not been fitted yet, please run fit first !')
        output_prob = self.feed_forward(X)
        output_lab = [1 if pred>self.treshold else 0 for pred in output_prob]

        return output_lab

```

**Complexit√© temporelle**

La m√©thode init a une complexit√© temporelle constante, elle ne stocke que des constantes ind√©pendantes des entr√©es et n‚Äôeffectue aucun calcul. 

La m√©thode sigmoid effectue une op√©ration d‚Äôexp, en consid√©rant cette op√©ration √©l√©mentaire pour chaque √©l√©ment du vecteur de taille $p$, on a donc au total $p$ op√©rations, donc une complexit√© en $O(p)$. 

La m√©thode feed_forward fait une multiplication matricielle dont la compl√©xit√© est $O(p¬≤n)$, une addition matricielle dont la compl√©xit√© est $O(p)$, et enfin la m√©thode sigmoid dont la complexite est $O(p)$. En addition ces compl√©xit√© est en tenant compte de l‚Äôordre de grandeur; cette m√©thode a une compl√©xit√© $O(p¬≤n)$.  

La m√©thode compute_loss prend en entr√©e deux vecteurs de m√™me taille $p$. Il y a quatre op√©rations  addition/soustraction, et deux multiplications vectorielles (√©l√©ment par √©l√©ment) entre vecteurs dont la complexit√© est $O(p)$. L‚Äôop√©ration np.log est constante, sur un vecteur de taille p, sa complexit√© est $O(p)$. En additionnant ces complexit√©s, on obtient une complexit√© $O(p)$. 

La m√©thode fit, la premi√®re ligne est une affectation pas de calcul. Examinons un tour de boucle :

La premi√®re instruction est un appel √† feed_forward dont la complexit√© est en $O(p¬≤n)$. Ensuite, une multiplication matricielle (matrice-vecteur) dont la complexit√© est la m√™me que la m√©thode feed_forward. La fonction np.mean de numpy a une complexit√© lin√©aire $O(p)$, les autres op√©rations d‚Äôadditions et multiplications sont constantes. Pour la mise √† jour de w, on a $O(p)$ en termes de complexit√©. En additionnant, on obtient une complexit√© $O(p¬≤n)$. 

Pour un nombre $m$ d‚Äôit√©rations, on aura $m*O(p¬≤n)$, et donc la complexit√© de cette m√©thode fit est de $O(mnp¬≤)$. 

Pour la m√©thode predict, on a un appel √† feed_forward et une boucle sur chaque √©l√©ment du vecteur de probabilit√© de taille $p$, donc cette m√©thode a une complexit√© $O(p¬≤n)$. 

**Compl√©xit√© spatiale**

La m√©thode **init** a une complexit√© constante, elle ne stocke que des constantes. La m√©thode _sigmoid_function a besoin de stocker le r√©sultat, dont un vecteur de taille $p$. La complexit√© est en $O(p)$. Pour la m√©thode feed_forward, il faut stocker le vecteur z de taille $p$, et le r√©sultat qui est de taille $p$, donc $2p$, la complexit√© est de $O(p)$. La m√©thode compute_loss stocke les deux variables res1 et res2, ainsi que le r√©sultat, chacune a besoin d‚Äôune allocation m√©moire de $p$, donc une complexit√© de $O(p)$. 

La m√©thode **fit** avant ****la boucle ne stocke que deux variables dont l‚Äôespace m√©moire est de taille p. Les six variables dans la boucle ont besoin chacune d‚Äôun espace m√©moire de p, donc la complexit√© pour un tour de boucle est $O(p)$, donc pour m tour de boucle, on aura une complexit√© finale $O(mp)$. Easy !

**Conclusion**

L‚Äô√©tude de la complexit√© ne peut √™tre r√©duite √† ce blog. Le but ici √©tait de vous donner quelques √©l√©ments cl√©s pour √©valuer la complexit√© d‚Äôun algorithme de machine learning ou non. La complexit√© ne remplace pas le benchmark, mais c‚Äôest un outil qui permet d‚Äôavoir une id√©e du comportement d‚Äôun algorithme ind√©pendamment du langage d‚Äôimpl√©mentation et des d√©tails d‚Äôoptimisation li√©s √† ce langage. La constante que cache la notation  peut √™tre trompeuse, puisqu‚Äôelle peut √™tre tr√®s grande et qu‚Äôen pratique, un algorithme de complexit√© polynomiale soit plus rapide qu‚Äôun algorithme de complexit√© logarithmique. Le Quick sort n‚Äôest pas le meilleur algorithme de tri en ce qui concerne la complexit√©, mais il est tout de m√™me le meilleur en pratique et reste un choix par d√©faut.